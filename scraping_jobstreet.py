# -*- coding: utf-8 -*-
"""scraping jobstreet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lmJbHiwPrO0RcATrLlTxZFsh9IMEpIAh

# Page 2
"""

def scrape_article_ids(api_url, page_number):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]

    page_url = f'{api_url}&page={page_number}'

    # Send an HTTP request to the API endpoint
    response = requests.get(page_url)  # Use page_url here
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()

        # Extract advertiser IDs from each item in the 'data' list
        for item in data['data']:
            jid = item['id']
            title = item['title']
            company = item['advertiser'].get('description', '')
            location = item.get('location', '')
            category = item['classification'].get('description', '')
            subCategory= item['subClassification'].get('description', '')
            job_type = item.get('workType', '')
            salary = item.get('salary', '')

            job_id.append(jid)
            titles.append(title)
            companies.append(company)
            locations.append(location)
            categorys.append(category)
            subCategorys.append(subCategory)
            job_types.append(job_type)
            salarys.append(salary)

    else:
        print(f"Gagal mengambil data dari API. Status Code: {response.status_code}")

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, 2)  # Change this to 7 to scrape page 7 only

# The rest of your code...

def scrape_article_ids(api_url, page_number):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]

    page_url = f'{api_url}&page={page_number}'

    # Send an HTTP request to the API endpoint
    response = requests.get(page_url)  # Use page_url here
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()

        # Extract advertiser IDs from each item in the 'data' list
        for item in data['data']:
            jid = item['id']
            title = item['title']
            company = item['advertiser'].get('description', '')
            location = item.get('location', '')
            category = item['classification'].get('description', '')
            subCategory= item['subClassification'].get('description', '')
            job_type = item.get('workType', '')
            salary = item.get('salary', '')

            job_id.append(jid)
            titles.append(title)
            companies.append(company)
            locations.append(location)
            categorys.append(category)
            subCategorys.append(subCategory)
            job_types.append(job_type)
            salarys.append(salary)

    else:
        print(f"Gagal mengambil data dari API. Status Code: {response.status_code}")

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, 2)  # Change this to 7 to scrape page 7 only

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data


data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_article_ids(api_url, page_number):
    job_data = []

    page_url = f'{api_url}&page={page_number}'

    # Send an HTTP request to the API endpoint
    response = requests.get(page_url)
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()

        # Extract information from each item in the 'data' list
        for item in data['data']:
            job_info = {}
            job_info['job_id'] = item.get('id', '')
            job_info['title'] = item.get('title', '')
            job_info['company'] = item.get('advertiser', {}).get('description', '')
            job_info['location'] = item.get('location', '')
            job_info['category'] = item.get('classification', {}).get('description', '')
            job_info['subcategory'] = item.get('subClassification', {}).get('description', '')
            job_info['job_type'] = item.get('workType', '')
            job_info['salary'] = item.get('salary', '')
            job_data.append(job_info)
    else:
        print(f"Failed to retrieve data from API. Status Code: {response.status_code}")

    return job_data

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def extract_education_from_description(description):
    # Assuming education information is between "Education:" and the next bullet point or end of string
    start_idx = description.find("Education:") + len("Education:")
    end_idx = description.find("\nâ€¢", start_idx)
    if end_idx == -1:
        end_idx = len(description)
    education_info = description[start_idx:end_idx].strip()
    return education_info

def scrape_and_store_text(job_data):
    data = {'job_id': [], 'title': [], 'company': [], 'descriptions': [], 'location': [], 'category': [],
            'subcategory': [], 'job_type': [], 'salary': [], 'education': []}

    for job_info in job_data:
        job_article_content = fetch_job_article(job_info['job_id'])

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_info['job_id'])
            data['descriptions'].append(text_from_ul)
            data['title'].append(job_info['title'])
            data['company'].append(job_info['company'])
            data['location'].append(job_info['location'])
            data['category'].append(job_info['category'])
            data['subcategory'].append(job_info['subcategory'])
            data['job_type'].append(job_info['job_type'])
            data['salary'].append(job_info['salary'])
            data['education'].append(extract_education_from_description(text_from_ul))

    return data

# Example API URL
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'

# Scrape article IDs from the API
job_data = scrape_article_ids(api_url, 2)  # Change this to 7 to scrape page 7 only

# Scrape and store text for each job article
data = scrape_and_store_text(job_data)

# Create DataFrame from the scraped data
result_df = pd.DataFrame(data)
result_df

"""# PAGE 1"""

def scrape_article_ids(api_url, page_number):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]

    page_url = f'{api_url}&page={page_number}'

    # Send an HTTP request to the API endpoint
    response = requests.get(page_url)  # Use page_url here
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()

        # Extract advertiser IDs from each item in the 'data' list
        for item in data['data']:
            jid = item['id']
            title = item['title']
            company = item['advertiser'].get('description', '')
            location = item.get('location', '')
            category = item['classification'].get('description', '')
            subCategory= item['subClassification'].get('description', '')
            job_type = item.get('workType', '')
            salary = item.get('salary', '')

            job_id.append(jid)
            titles.append(title)
            companies.append(company)
            locations.append(location)
            categorys.append(category)
            subCategorys.append(subCategory)
            job_types.append(job_type)
            salarys.append(salary)

    else:
        print(f"Gagal mengambil data dari API. Status Code: {response.status_code}")

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, 1)  # Change this to 7 to scrape page 7 only

# The rest of your code...

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data


data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

"""# PAGE 3"""

pip install requests

import requests

def scrape_article_ids(api_url, page_number):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]

    page_url = f'{api_url}&page={page_number}'

    # Send an HTTP request to the API endpoint
    response = requests.get(page_url)  # Use page_url here
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()
        print(data)

        # Extract advertiser IDs from each item in the 'data' list
        for item in data['data']:
            jid = item['id']
            title = item['title']
            company = item['advertiser'].get('description', '')
            location = item.get('location', '')
            category = item['classification'].get('description', '')
            subCategory= item['subClassification'].get('description', '')
            job_type = item.get('workType', '')
            salary = item.get('salary', '')

            job_id.append(jid)
            titles.append(title)
            companies.append(company)
            locations.append(location)
            categorys.append(category)
            subCategorys.append(subCategory)
            job_types.append(job_type)
            salarys.append(salary)

    else:
        print(f"Gagal mengambil data dari API. Status Code: {response.status_code}")

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, 3)  # Change this to 7 to scrape page 7 only

# The rest of your code...

from bs4 import BeautifulSoup
import pandas as pd

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data


data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

"""# PAGE 4"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
pd.set_option('display.max_colwidth', 200)

import time

def scrape_article_ids(api_url, page_number):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]

    page_url = f'{api_url}&page={page_number}'

    # Send an HTTP request to the API endpoint
    response = requests.get(page_url)  # Use page_url here
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()
        print(data)

        # Check if 'data' key exists in the response
        if 'data' in data:
            # Extract advertiser IDs from each item in the 'data' list
            for item in data['data']:
                try:
                    jid = item['id']
                    title = item['title']
                    company = item['advertiser'].get('description', '')
                    location = item.get('location', '')
                    category = item['classification'].get('description', '')
                    subCategory= item['subClassification'].get('description', '')
                    job_type = item.get('workType', '')
                    salary = item.get('salary', '')

                    job_id.append(jid)
                    titles.append(title)
                    companies.append(company)
                    locations.append(location)
                    categorys.append(category)
                    subCategorys.append(subCategory)
                    job_types.append(job_type)
                    salarys.append(salary)
                except Exception as e:
                    print(f"Error parsing item: {e}")
        else:
            print("No 'data' key in the response")

    else:
        print(f"Gagal mengambil data dari API. Status Code: {response.status_code}")

    # Sleep for a while to avoid rate limiting
    time.sleep(1)

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, 4)  # Change this to 7 to scrape page 7 only

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data


data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

"""# YANG BARUUUUU"""

from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()

driver.get("https://www.jobstreet.co.id/id/data-jobs?")

import pandas as pd
import os
import pandas_gbq
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from config import BASE_URL, CHROMEDRIVER_PATH, KEYWORDS, SIBLING_PARENT_XPATH, LOG_DIR, LOG_INFO_PATH, LOG_INFO_FILEMODE, BIGQUERY_TABLE_NAME
from helpers import time_difference_fmt

from dotenv import load_dotenv
load_dotenv()

from selenium.webdriver.chrome.options import Options
options = Options()
# options.add_argument("--headless")
options.add_argument("window-size=1920,1080")

from pathlib import Path
Path(LOG_DIR).mkdir(exist_ok=True)

import logging
logging.basicConfig(level=logging.INFO, filename=LOG_INFO_PATH, filemode=LOG_INFO_FILEMODE)

class JobScrapper(object):
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.driver = webdriver.Chrome(chrome_options=options, executable_path=CHROMEDRIVER_PATH)
        self.list_company_name = []
        self.list_job_posting_time = []
        self.list_career_level = []
        self.list_company_size = []
        self.list_company_industry = []
        self.list_company_description = []
        self.list_employment_type = []
        self.list_job_function = []

    def begin_scrap(self):
        for keyword in KEYWORDS:
            self.driver.get(BASE_URL)
            WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable((By.ID, 'searchKeywordsField'))).send_keys(keyword)

            search_element = self.driver.find_element(By.ID, 'searchKeywordsField')
            search_element.send_keys(Keys.ENTER) #input the keyword to search bar

            WebDriverWait(self.driver, 10).until(
                EC.presence_of_all_elements_located((By.ID, 'jobList')))

            pagination_elements = self.driver.find_elements(By.ID, 'pagination')

            if len(pagination_elements) > 0:  # pagination element exist
                pag_element = pagination_elements[0]
                option_elements = pag_element.find_elements(
                    By.XPATH, '//*[@id="pagination"]/option')
                max_page = int(option_elements[-1].text)
                self.logger.info(f"{keyword} keyword search results max page: {max_page}")

                base_job_url = self.driver.current_url

                for page in range(1, max_page+1):
                    self.logger.info(f"Current Page: {base_job_url}{page}")
                    self.driver.get(f"{base_job_url}{page}")
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_all_elements_located((By.ID, 'jobList')))
                    article_elements = self.driver.find_elements(By.XPATH, '//article')

                    for i, article_element in enumerate(article_elements):
                        time_element = article_element.find_element(By.TAG_NAME, 'time')
                        job_post_iso_time = time_element.get_attribute("datetime")
                        job_posted_time = time_difference_fmt(posted_time=job_post_iso_time)
                        self.list_job_posting_time.append(job_posted_time)

                        job_card_element = self.driver.find_element(By.XPATH, f"//article[@data-automation='job-card-{i}']//div//h1/a")
                        self.driver.execute_script("arguments[0].scrollIntoView(true)", job_card_element)
                        self.driver.execute_script("arguments[0].click()", job_card_element)

                        WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located((By.XPATH, "//div[@data-automation='splitModeJobDetailsScrollWrapper']"))
                        )

                        self.scrap_jobs_data()
        self.logger.info('Scrapping process Done!')

        #Put together to Pandas Dataframe then Store to BigQuery
        df = pd.DataFrame(
            {
                'company_name': self.list_company_name,
                'job_posting_time': self.list_job_posting_time,
                'career_level': self.list_career_level,
                'size_of_company': self.list_company_size,
                'company_industry': self.list_company_industry,
                'detail_description': self.list_company_description,
                'employment_type': self.list_employment_type,
                'job_function': self.list_job_function
            }
        )

        df.to_csv('jobstreet_scrap_result.csv', index=False)
        self.store_to_bigquery(df)

    def scrap_jobs_data(self):
        WebDriverWait(self.driver, 10).until(
            EC.visibility_of_all_elements_located((By.XPATH, "//div[@data-automation='detailsTitle']//span"))
        )

        company_name_element = self.driver.find_element(By.XPATH, "//div[@data-automation='detailsTitle']//span")
        company_name_text = company_name_element.text
        self.list_company_name.append(company_name_text)
        self.logger.info(f"Now scrap company name: {company_name_text}")

        company_description_element = self.driver.find_element(By.XPATH, "//div[@data-automation='jobDescription']")
        company_description_text = company_description_element.text

        job_highlight_text = self.get_additional_information(
            selector_to_check="//div[@data-automation='job-details-job-highlights']",
            text_selector="//div[@data-automation='job-details-job-highlights']",
            text='Job Highlight'
        )

        benefit_text = self.get_additional_information(
            selector_to_check="//span[text() = 'Benefits & Others']",
            text_selector=SIBLING_PARENT_XPATH,
            is_sibling=True,
            text='Benefits'
        )
        final_description = f"{company_description_text}\n\n{job_highlight_text}\n\nBenefits & Others:\n{benefit_text}"
        self.list_company_description.append(final_description)

        career_level_text = self.get_additional_information(
            selector_to_check="//span[text() = 'Career Level']",
            text_selector=SIBLING_PARENT_XPATH,
            is_sibling=True,
            text='Career Level'
        )
        self.list_career_level.append(career_level_text)

        job_function_text = self.get_additional_information(
            selector_to_check="//span[text() = 'Job Specializations']",
            text_selector=SIBLING_PARENT_XPATH,
            is_sibling=True,
            text='Job Function'
        )
        self.list_job_function.append(job_function_text)

        employment_type_text = self.get_additional_information(
            selector_to_check="//span[text() = 'Job Type']",
            text_selector=SIBLING_PARENT_XPATH,
            is_sibling=True,
            text='Employment Type'
        )
        self.list_employment_type.append(employment_type_text)

        company_size_text = self.get_additional_information(
            selector_to_check="//span[text() = 'Company Size']",
            text_selector=SIBLING_PARENT_XPATH,
            is_sibling=True,
            text='Company Size'
        )
        self.list_company_size.append(company_size_text)

        industry_text = self.get_additional_information(
            selector_to_check="//span[text() = 'Industry']",
            text_selector=SIBLING_PARENT_XPATH,
            is_sibling=True,
            text='Company Industry'
        )
        self.list_company_industry.append(industry_text)

    def get_additional_information(self, selector_to_check, text_selector, text, is_sibling=False):
        prop_elements = self.driver.find_elements(By.XPATH, selector_to_check)
        if len(prop_elements) >= 1:
            if is_sibling is True:
                sibling_element = prop_elements[-1].find_element(By.XPATH, text_selector)
                information_text = sibling_element.text
            else:
                element = self.driver.find_element(By.XPATH, text_selector)
                information_text = element.text
        else:
            information_text = f'No {text}'

        return information_text

    def store_to_bigquery(self, df):
        self.logger.info('Writing to BigQuery...')
        dataset_id = os.getenv('DATASET_ID')
        project_id = os.getenv('PROJECT_ID')
        pandas_gbq.to_gbq(
            df, f"{dataset_id}.{BIGQUERY_TABLE_NAME}",
            project_id=project_id, if_exists="replace"
        )
        self.logger.info('SUCCESS writing to BigQuery')

!pip install wordcloud contractions

from bs4 import BeautifulSoup
import requests
import pandas as pd

html_text = requests.get(f"https://www.jobstreet.co.id/").text
web_html = BeautifulSoup(html_text, "lxml")

job = web_html.find_all("div", class_="sx2jih0 _2j8fZ_0 sIMFL_0 _1JtWu_0")
company = web_html.find_all("span", class_="sx2jih0 zcydq82q _18qlyvc0 _18qlyvcv _18qlyvc1 _18qlyvc8")
location = web_html.find_all("span", class_="sx2jih0 zcydq82q zcydq810 iwjz4h0")
salary = web_html.find_all("span", class_="sx2jih0 zcydq82q _18qlyvc0 _18qlyvcv _18qlyvc3 _18qlyvc6")
published = web_html.find_all("span", class_="sx2jih0 zcydq82q _18qlyvc0 _18qlyvcx _18qlyvc1 _18qlyvc6")

data = []
for x in range(len(job)):
    row = {}
    row["Job"] = job[0+x].text
    row["Company Name"] = company[21+x].text
    row["Location"] = location[3+x].text
    row["Salary"] = salary[1+x].text if 'IDR' in salary[1+x].text else '-'
    row["Published"] = published[0+x].text
    data.append(row)

df = pd.DataFrame(data)
df

web_html

df.to_csv("job_data.csv", index=False)

import requests # for web requests
from bs4 import BeautifulSoup # a powerful HTML parser
import pandas as pd # for .csv file read and write
import re # for regular regression handling

headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'}


""" get all position <a> tags for the list of job roles, results stored in a dictionary
<a> tag example:
<a class="position-title-link" id="position_title_3" href="https://www.jobstreet.com.sg/en/job/data-analyst-python-sas-sqlbank-35k-to-5k-gd-bonus5-days-west-6111488?fr=21"
target="_blank" title="View Job Details - Data Analyst (Python / SAS / SQL)(BANK / $3.5K to $5K + GD BONUS / 5 Days / West)" data-track="sol-job" data-job-id="6111488"
data-job-title="Data Analyst (Python / SAS / SQL)(BANK / $3.5K to $5K + GD BONUS / 5 Days / West)" data-type="organic" data-rank="3" data-page="1" data-posting-country="SG">
<h2 itemprop="title">Data Analyst (Python / SAS / SQL)(BANK / $3.5K to $5K + GD BONUS / 5 Days / West)</h2></a>"""
def linksByKeys(keys):
    ## keys: a list of job roles
    ## return: a dictionary of links

    links_dic = dict()
    # scrape key words one by one
    for key in keys:
        print('Scraping position: ', key, ' ...')
        links_dic[key] = linksByKey(key)
        print('{} {} positions found!'.format(len(links_dic[key]),key))
    return links_dic


""" get all position <a> tags for a single job role, triggered by linksByKeys function """
def linksByKey(key):
    ## key: a job role
    ## return: a list of links

    # parameters passed to  http get/post function
    base_url = 'https://www.jobstreet.co.id/'
    pay_load = {'key':'','area':1,'option':1,'pg':None,'classified':1,'src':16,'srcr':12}
    pay_load['key'] = key

    # page number
    pn = 2

    position_links = []
    loaded = True
    while loaded:
        print('Loading page {} ...'.format(pn))
        pay_load['pg'] = pn
        r = requests.get(base_url, headers=headers, params=pay_load)

        # extract position <a> tags
        soup = BeautifulSoup(r.text,'html.parser')
        links = soup.find_all('a',{'class':'position-title-link','data-job-id':re.compile(r'.*')})

        # if return nothing, means the function reach the last page, return results
        if not len(links):
            loaded = False
        else:
            position_links += links
            pn += 1
    return position_links


""" parse HTML strings for the list of roles
<a> tag example:
<a class="position-title-link" id="position_title_3" href="https://www.jobstreet.com.sg/en/job/data-analyst-python-sas-sqlbank-35k-to-5k-gd-bonus5-days-west-6111488?fr=21"
target="_blank" title="View Job Details - Data Analyst (Python / SAS / SQL)(BANK / $3.5K to $5K + GD BONUS / 5 Days / West)" data-track="sol-job" data-job-id="6111488"
data-job-title="Data Analyst (Python / SAS / SQL)(BANK / $3.5K to $5K + GD BONUS / 5 Days / West)" data-type="organic" data-rank="3" data-page="1" data-posting-country="SG">"""
def parseLinks(links_dic):
    ## links_dic: a dictionary of links
    ## return: print parsed results to .csv file

    for key in links_dic:
        jobs = []
        for link in links_dic[key]:
            jobs.append([key] + parseLink(link))

        # transfrom the result to a pandas.DataFrame
        result = pd.DataFrame(jobs,columns=['key_word','job_id','job_title','country','job_link','company','company_region','company_industry','company_size','experence_requirement','working_location','description'])

        # add a column denoting if the position is posted by a recuriter company
        result['postedByHR'] = result.company_industry.apply(lambda x:True if x and x.find('Human Resources')>-1 else False)

        # save result,
        file_name = key+'.csv'
        result.to_csv(file_name,index=False)


""" parse a single <a> tag, extract the information, triggered by parseLinks function """
def parseLink(link):
	## link: a single position <a> tag
	## return: information of a single position

	# unique id assigned to a position
	job_id = link['data-job-id'].strip()
	# job title
	job_title = link['data-job-title'].strip()
	# posted country
	country = link['data-posting-country'].strip()
	# the web address towards to the post detail page
	job_href = link['href']
	# go to post detail page, and fetch information
	other_detail = getJobDetail(job_href)
	return [job_id,job_title,country,job_href] + other_detail


""" extract details from post detail page """
def getJobDetail(job_href):
    ## job_href: a post url
    ## retun: post details from the detail page

    print('Scraping ',job_href,'...')
    r = requests.get(job_href)
    soup = BeautifulSoup(r.text,'html.parser')
    # company who posts the position, very often is a recuriter company
    company_name = soup.find('div',{'id':'company_name'}).text.strip() if soup.find('div',{'id':'company_name'}) else None
    # years of working experience required
    years_of_experience= soup.find('span',{'id':'years_of_experience'}).text.strip() if soup.find('span',{'id':'years_of_experience'}) else None
    # location of the company
    company_location = soup.find('span',{'id':'single_work_location'}).text.strip() if soup.find('span',{'id':'single_work_location'}) else None
    # industry of the company who posts the position, very often is a recuriter company
    company_industry = soup.find('p',{'id':'company_industry'}).text.strip() if soup.find('p',{'id':'company_industry'}) else None
    # size of the company who posts the position
    company_size = soup.find('p',{'id':'company_size'}).text.strip() if soup.find('p',{'id':'company_size'}) else None
    # working location
    job_location = soup.find('p',{'id':'address'}).text.strip() if soup.find('p',{'id':'address'}) else None
    # job description, which contains information about job scope, requirements and sometimes a brief introduction about the comapny
    job_description = soup.find('div',{'id':'job_description'}).text.strip() if soup.find('div',{'id':'job_description'}) else None
    return [company_name,company_location,company_industry,company_size,years_of_experience,job_location,job_description]

def main():

    # a list of job roles to be crawled
    key_words = ['data']
    s = requests.session()
    links_dic = linksByKeys(key_words)
    parseLinks(links_dic)

if __name__ == '__main__':
	main()



"""# COBA LAGI"""

def scrape_article_ids(api_url, max_pages):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]
    for page_number in range(1, max_pages + 1):
        page_url = f'{api_url}&page={page_number}'
        print(f"Accessing URL: {page_url}")  # Print the URL

        # Send an HTTP request to the API endpoint
        response = requests.get(page_url)  # Use page_url here
        if response.status_code == 200:
            # Parse the JSON response
            data = response.json()

            # Extract advertiser IDs from each item in the 'data' list
            for item in data['data']:
                jid = item['id']
                title = item['title']
                company = item['advertiser'].get('description', '')
                location = item.get('location', '')
                category = item['classification'].get('description', '')
                subCategory= item['subClassification'].get('description', '')
                job_type = item.get('workType', '')
                salary = item.get('salary', '')

                job_id.append(jid)
                titles.append(title)
                companies.append(company)
                locations.append(location)
                categorys.append(category)
                subCategorys.append(subCategory)
                job_types.append(job_type)
                salarys.append(salary)

        else:
            print(f"Failed to retrieve data from the API. Status Code: {response.status_code}")
            break

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys

max_pages = 5

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data+scientist&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, max_pages)

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data

# print("Length of job_ids:", len(job_ids))
# print("Length of job_types:", len(job_types))
# print("Length of salarys:", len(salarys))

data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

df = result_df

duplicate_rows_data = df[df.duplicated()]
print("Jumlah baris yang terduplikat: ", duplicate_rows_data.shape[0])
print("Baris yang terduplikat:")
print(duplicate_rows_data)

result_df.to_excel('jobstreet.xlsx', index=False)

"""# COBA LAGI PART 2"""

def scrape_article_ids(api_url, max_pages):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]
    for page_number in range(1, max_pages + 1):
        page_url = f'{api_url}&page={page_number}'

        # Send an HTTP request to the API endpoint
        response = requests.get(page_url)  # Use page_url here
        if response.status_code == 200:
            # Parse the JSON response
            data = response.json()

            # Extract advertiser IDs from each item in the 'data' list
            for item in data['data']:
                jid = item['id']
                title = item['title']
                company = item['advertiser'].get('description', '')
                location = item.get('location', '')
                category = item['classification'].get('description', '')
                subCategory= item['subClassification'].get('description', '')
                job_type = item.get('workType', '')
                salary = item.get('salary', '')

                job_id.append(jid)
                titles.append(title)
                companies.append(company)
                locations.append(location)
                categorys.append(category)
                subCategorys.append(subCategory)
                job_types.append(job_type)
                salarys.append(salary)

        else:
            print(f"Failed to retrieve data from the API. Status Code: {response.status_code}")
            break

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys


max_pages = 3

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data+scientist&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b&page=5'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, max_pages)

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data

# print("Length of job_ids:", len(job_ids))
# print("Length of job_types:", len(job_types))
# print("Length of salarys:", len(salarys))

data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

"""# YANG BARU"""

def scrape_article_ids(api_url, max_pages):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]
    for page_number in range(1, max_pages + 1):
        page_url = f'{api_url}&page={page_number}'

        # Send an HTTP request to the API endpoint
        response = requests.get(page_url)  # Use page_url here
        if response.status_code == 200:
            # Parse the JSON response
            data = response.json()

            # Extract advertiser IDs from each item in the 'data' list
            for item in data['data']:
                jid = item['id']
                title = item['title']
                company = item['advertiser'].get('description', '')
                location = item.get('location', '')
                category = item['classification'].get('description', '')
                subCategory= item['subClassification'].get('description', '')
                job_type = item.get('workType', '')
                salary = item.get('salary', '')

                job_id.append(jid)
                titles.append(title)
                companies.append(company)
                locations.append(location)
                categorys.append(category)
                subCategorys.append(subCategory)
                job_types.append(job_type)
                salarys.append(salary)

        else:
            print(f"Failed to retrieve data from the API. Status Code: {response.status_code}")
            break

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys


max_pages = 3

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data+scientist&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, max_pages)

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data

# print("Length of job_ids:", len(job_ids))
# print("Length of job_types:", len(job_types))
# print("Length of salarys:", len(salarys))

data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

result_df.to_excel('jobstreet1.xlsx', index=False)

!pip install selenium

import pandas as pd
from bs4 import BeautifulSoup
from selenium.webdriver import Chrome
import re
import time
import json
import math

headers = {
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}
from selenium import webdriver


path = "/Users/peiming/Downloads/chromedriver-2"
driver = webdriver.Chrome(executable_path=path)


time.sleep(2)
base_url = "https://www.jobstreet.com.sg/en/job-search/{}-jobs/{}/"


def get_page_number(keyword):
    # input: keyword for job_postings
    # output: number of pages

    url = base_url.format(keyword, 1)
    driver.get(url)
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    #result_text = soup.find("span", {"class": "sx2jih0 zcydq84u _18qlyvc0 _18qlyvc1x _18qlyvc1 _1d0g9qk4 _18qlyvc8"})
    result_text = soup.find('span',{'class': "sx2jih0 zcydq84u es8sxo0 es8sxo1 es8sxo21 _1d0g9qk4 es8sxo7"})
    results = result_text.text.split()
    result = result_text.text.split()[-2]
    result = result.replace(',', '')
    page_number = math.ceil(int(result) / 30)

    return page_number


def job_page_scraper(link):
    url = "https://www.jobstreet.com.sg" + link
    print("scraping...", url)
    driver.get(url)
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    scripts = soup.find_all("script")

    for script in scripts:
        if script.contents:
            txt = script.contents[0].strip()
            if 'window.REDUX_STATE = ' in txt:
                jsonStr = script.contents[0].strip()
                jsonStr = jsonStr.split('window.REDUX_STATE = ')[1].strip()
                jsonStr = jsonStr.split('}}}};')[0].strip()
                jsonStr = jsonStr + "}}}}"
                jsonObj = json.loads(jsonStr)

    job = jsonObj['details']
    job_id = job['id']
    job_expired = job['isExpired']
    job_confidential = job['isConfidential']
    job_salary_min = job['header']['salary']['min']
    job_salary_max = job['header']['salary']['max']
    job_salary_currency = job['header']['salary']['currency']
    job_title = job['header']['jobTitle']
    company = job['header']['company']['name']
    job_post_date = job['header']['postedDate']
    job_internship = job['header']['isInternship']
    company_website = job['companyDetail']['companyWebsite']
    company_avgProcessTime = job['companyDetail']['companySnapshot']['avgProcessTime']
    company_registrationNo = job['companyDetail']['companySnapshot']['registrationNo']
    company_workingHours = job['companyDetail']['companySnapshot']['workingHours']
    company_facebook = job['companyDetail']['companySnapshot']['facebook']
    company_size = job['companyDetail']['companySnapshot']['size']
    company_dressCode = job['companyDetail']['companySnapshot']['dressCode']
    company_nearbyLocations = job['companyDetail']['companySnapshot']['nearbyLocations']
    company_overview = job['companyDetail']['companyOverview']['html']
    job_description = job['jobDetail']['jobDescription']['html']
    job_summary = job['jobDetail']['summary']
    job_requirement_career_level = job['jobDetail']['jobRequirement']['careerLevel']
    job_requirement_yearsOfExperience = job['jobDetail']['jobRequirement']['yearsOfExperience']
    job_requirement_qualification = job['jobDetail']['jobRequirement']['qualification']
    job_requirement_fieldOfStudy = job['jobDetail']['jobRequirement']['fieldOfStudy']
    # job_requirement_industry = job['jobDetail']['jobRequirement']['industryValue']['label']
    job_requirement_skill = job['jobDetail']['jobRequirement']['skills']
    job_employment_type = job['jobDetail']['jobRequirement']['employmentType']
    job_languages = job['jobDetail']['jobRequirement']['languages']
    job_benefits = job['jobDetail']['jobRequirement']['benefits']
    job_apply_url = job['applyUrl']['url']
    job_location_zipcode = job['location'][0]['locationId']
    job_location = job['location'][0]['location']
    job_country = job['sourceCountry']

    return [job_id, job_title, job_expired, job_confidential, job_salary_max, job_salary_max, job_salary_currency,
            company, job_post_date, job_internship, company_website, company_avgProcessTime, company_registrationNo,
            company_workingHours, company_facebook, company_size, company_dressCode, company_nearbyLocations,
            company_overview, job_description, job_summary, job_requirement_career_level, job_requirement_fieldOfStudy,
            job_requirement_yearsOfExperience, job_requirement_qualification, job_requirement_skill,
            job_employment_type, job_languages, job_benefits, job_apply_url, job_location_zipcode, job_location,
            job_country]


def page_crawler(keyword):
    # input: keyword for job postings
    # output: dataframe of links scraped from each page

    # page number
    page_number = get_page_number(keyword)
    job_links = []

    for n in range(71):
        print('Loading page {} ...'.format(n))
        url = base_url.format(keyword, n)
        driver.get(url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # extract all job links
        links = soup.find_all('a', {'class' :"_1hr6tkx5 _1hr6tkx7 _1hr6tkxa sx2jih0 sx2jihf zcydq8h"})
        job_links += links

    jobs = []

    for link in job_links:
        job_link = link['href'].strip().split('?', 1)[0]
        if "job-search" not in job_link:
            jobs.append([keyword, job_link] + job_page_scraper(job_link))

    print(jobs)

    result_df = pd.DataFrame(jobs, columns=['keyword', 'link', 'job_id', 'job_title', 'job_expired', 'job_confidential',
                                            'job_salary_max', 'job_salary_max', 'job_salary_currency', 'company',
                                            'job_post_date', 'job_internship', 'company_website',
                                            'company_avgProcessTime', 'company_registrationNo', 'company_workingHours',
                                            'company_facebook', 'company_size', 'company_dressCode',
                                            'company_nearbyLocations', 'company_overview', 'job_description',
                                            'job_summary', 'job_requirement_career_level',
                                            'job_requirement_fieldOfStudy', 'job_requirement_yearsOfExperience',
                                            'job_requirement_qualification', 'job_requirement_skill',
                                            'job_employment_type', 'job_languages', 'job_benefits', 'job_apply_url',
                                            'job_location_zipcode', 'job_location', 'job_country'])
    return result_df


def main():
    # a list of job roles to be crawled
    key_words = ['"devops"']
    dfs = []

    for key in key_words:
        key_df = page_crawler(key)
        dfs.append(key_df)

    # save scraped information as csv
    pd.concat(dfs).to_csv("devops_71.csv")


if __name__ == '__main__':
    main()

"""# YANG LAMA"""

data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

import pandas as pd
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
pd.set_option('display.max_colwidth', 200)

def scrape_article_ids(api_url, max_pages):
    job_id =[]
    titles = []
    companies = []
    locations = []
    categorys= []
    subCategorys= []
    job_types=[]
    salarys=[]
    for page_number in range(1, max_pages + 1):
        page_url = f'{api_url}&page={page_number}'

        # Send an HTTP request to the API endpoint
        response = requests.get(api_url)
        if response.status_code == 200:
            # Parse the JSON response
            data = response.json()

            # Extract advertiser IDs from each item in the 'data' list
            for item in data['data']:
                #print(item)
                jid = item['id']
                title = item['title']
                company = item['advertiser'].get('description', '')
                location = item.get('location', '')
                category = item['classification'].get('description', '')
                subCategory= item['subClassification'].get('description', '')
                job_type = item.get('workType', '')
                salary = item.get('salary', '')

                job_id.append(jid)
                titles.append(title)
                companies.append(company)
                locations.append(location)
                categorys.append(category)
                subCategorys.append(subCategory)
                job_types.append(job_type)
                salarys.append(salary)
                #print(f"Job ID: {job_id}")

        else:
            print(f"Failed to retrieve data from the API. Status Code: {response.status_code}")
            break

    return job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys


max_pages = 3

# api url taken from Network -> Header
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&usersessionid=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&eventCaptureSessionId=e56c7e89-1d1a-42b9-b7a5-37c12653d6b9&seekSelectAllPages=true&keywords=data+scientist&pageSize=99&include=seodata&locale=en-ID&solId=568d2fe8-e8ef-4998-8e24-3e1ccfb1348b'
job_id, titles, companies, locations, categorys, subCategorys,job_types,salarys = scrape_article_ids(api_url, max_pages)

def fetch_job_article(job_id):
    article_url = f'https://www.jobstreet.co.id/job/{job_id}'
    response = requests.get(article_url)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Failed to retrieve job article. Status Code: {response.status_code}")
        return None

def extract_text_from_ul(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    ul_tags = soup.find_all('ul')
    text_list = [ul.get_text(separator='\n') for ul in ul_tags]
    return '\n'.join(text_list)

def scrape_and_store_text(job_ids):
    data = {'job_id': [],'job_title':[],'company':[], 'descriptions': [], 'location':[],'category':[],'subcategory':[],'type':[],'salary':[]}

    for job_id in job_ids:
        job_article_content = fetch_job_article(job_id)

        if job_article_content:
            text_from_ul = extract_text_from_ul(job_article_content)
            data['job_id'].append(job_id)
            data['descriptions'].append(text_from_ul)

    data['job_title'] = titles
    data['company'] = companies
    data['location'] = locations
    data['category'] = categorys
    data['subcategory'] = subCategorys
    data['type']=job_types
    data['salary']=salarys


    return data

# print("Length of job_ids:", len(job_ids))
# print("Length of job_types:", len(job_types))
# print("Length of salarys:", len(salarys))

data = scrape_and_store_text(job_id)
result_df = pd.DataFrame(data)
result_df

result_df.to_excel('jobstreet.xlsx', index=False)

for key, value in data.items():
    print(f"Length of {key}: {len(value)}")

"""#**YUHU**"""

import pandas as pd
import requests

# Mempersiapkan fungsi untuk melakukan web scraping
def scrape_article_ids(api_url, max_pages):
    job_id = []
    titles = []
    companies = []
    locations = []
    categorys = []
    subCategorys = []
    job_types = []
    salarys = []

    for page_number in range(1, max_pages + 1):
        page_url = f'{api_url}&page={page_number}'
        response = requests.get(page_url)

        if response.status_code == 200:
            data = response.json()

            for item in data['data']:
                salary = item.get('salary', '')
                if salary:  # Hanya jika salary tersedia
                    job_id.append(item['id'])
                    titles.append(item['title'])
                    company = item['advertiser'].get('description', '')
                    companies.append(company)
                    location = item.get('location', '')
                    locations.append(location)
                    category = item['classification'].get('description', '')
                    categorys.append(category)
                    subCategory = item['subClassification'].get('description', '')
                    subCategorys.append(subCategory)
                    job_type = item.get('workType', '')
                    job_types.append(job_type)
                    salarys.append(salary)
        else:
            print(f"Failed to retrieve data from the API. Status Code: {response.status_code}")
            break

    return pd.DataFrame({
        'Job ID': job_id,
        'Job Title': titles,
        'Company': companies,
        'Location': locations,
        'Category': categorys,
        'Subcategory': subCategorys,
        'Job Type': job_type,
        'Salary': salarys
    })

# Mendefinisikan URL API dan jumlah maksimum halaman yang akan di-scrape
api_url = 'https://www.jobstreet.co.id/api/chalice-search/v4/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=2b00edd417ec163434fca9421e24c97a-7428735&userid=e56c7e89-1d1a-42b5-b7f7-a1a94da58f1&usersessionid=6b78392a-8a499835&eineventCaptureSidnapTotal6-dayEarn eble87tfb79bakbunewebCaltonedoOKayedehinde32ddacftI1-1StospeatPagesportsessesesiorn_mappedBYie-67068foglessd3suffirmGEand100okenfADoo396455_tokenminus&keywords=data&pageSize=99&include=ve=dytaholata12889/'
max_pages = 5

# Melakukan scraping data dan menyimpannya dalam DataFrame
result_df = scrape_article_ids(api_url, max_pages)

# Menampilkan DataFrame yang dihasilkan
result_df

